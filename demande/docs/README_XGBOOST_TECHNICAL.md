# üè® Documentation Technique - Pr√©diction XGBoost du Taux d'Occupation

## Vue d'ensemble

Ce document d√©taille l'architecture technique et le fonctionnement interne du script `xgboost_to_prediction.py`, qui impl√©mente un syst√®me de pr√©diction du taux d'occupation (To) futur des h√¥tels utilisant l'algorithme XGBoost.

## Architecture du syst√®me

### 1. Structure des classes

#### Classe `ToPredictor`
Classe principale orchestrant tout le processus de pr√©diction.

**Attributs :**
- `clustering_results_path` : Chemin vers le fichier CSV des r√©sultats de clustering
- `indicateurs_path` : Chemin vers le dossier contenant les fichiers indicateurs
- `model` : Mod√®le XGBoost entra√Æn√© (None si non entra√Æn√©)
- `scaler` : Objet StandardScaler pour la normalisation
- `feature_columns` : Liste des noms des colonnes de features utilis√©es

**M√©thodes principales :**
- `load_data()` : Chargement et validation des donn√©es
- `prepare_features()` : Pr√©paration des features d'entra√Ænement
- `train_model()` : Entra√Ænement du mod√®le XGBoost
- `predict_future_to()` : Pr√©diction pour de nouvelles donn√©es
- `plot_feature_importance()` : Analyse et visualisation des features importantes
- `save_model()` : Sauvegarde du mod√®le entra√Æn√©

## Algorithme de pr√©diction

### 1. Probl√®me de pr√©diction

**Type :** R√©gression supervis√©e
**Entr√©e :** Donn√©es historiques d'un h√¥tel sur 60 jours + m√©triques actuelles
**Sortie :** Taux d'occupation pr√©dit √† J+N jours (N configurable, d√©faut 7)

### 2. Features utilis√©es

#### Features temporelles (Courbes To)
- `J-60` √† `J-0` : Valeurs du taux d'occupation pour chaque jour des 60 derniers jours
- **Raison :** Capturer les patterns saisonniers et les tendances d'√©volution

#### Features √©conomiques (valeurs actuelles)
- `Pm_current` : Prix Moyen actuel (une seule valeur)
- `RevPAR_current` : Revenue Per Available Room actuel (une seule valeur)
- **Raison :** Indicateurs √©conomiques de r√©f√©rence pour la pr√©diction

#### Features cat√©gorielles
- `cluster` : Appartenance au cluster identifi√© par l'algorithme de clustering
- **Raison :** Diff√©rents types d'h√¥tels suivent des patterns diff√©rents

### 3. Variable cible (Target)
**Formule :** `To_futur = PM_futur / RevPAR_futur`
- Calcul√©e pour la date J + `prediction_horizon`
- Plafonn√©e √† 200% pour √©viter les valeurs aberrantes
- Fallback sur To actuel si donn√©es futures indisponibles

## Pipeline de traitement des donn√©es

### Phase 1 : Chargement des donn√©es

```python
def load_data(self):
    # 1. Charger clustering_results.csv
    cluster_df = pd.read_csv(self.clustering_results_path, sep=';')

    # 2. Charger indicateurs.csv
    indicateurs_df = pd.read_csv(f"{self.indicateurs_path}/Indicateurs.csv", sep=';')

    # 3. Conversion des dates
    cluster_df['stay_date'] = pd.to_datetime(cluster_df['stay_date'])
    indicateurs_df['Date'] = pd.to_datetime(indicateurs_df['Date'])
    indicateurs_df['ObsDate'] = pd.to_datetime(indicateurs_df['ObsDate'])

    return cluster_df, indicateurs_df
```

### Phase 2 : Pr√©paration des features

#### Strat√©gie principale
Pour chaque date de s√©jour dans les r√©sultats de clustering :

1. **Fusion des donn√©es :**
   - Jointure entre r√©sultats de clustering et donn√©es indicateurs
   - Utilisation des valeurs PM et RevPAR de la date de s√©jour

2. **Construction des features :**
   - Courbe To compl√®te (J-60 √† J-0)
   - Valeur unique PM actuelle
   - Valeur unique RevPAR actuelle
   - Cluster d'appartenance

3. **Calcul de la cible :**
   - Recherche des donn√©es PM/RevPAR pour J + `prediction_horizon`
   - Calcul : `target_to = PM_futur / RevPAR_futur`

#### Strat√©gie alternative (fallback)
Si moins de 100 √©chantillons valides :
- Utilisation des patterns moyens par cluster
- Pr√©diction bas√©e sur l'√©volution historique du cluster

### Phase 3 : Nettoyage et validation

```python
# Suppression des lignes avec cible manquante
features_df = features_df.dropna(subset=['target_to'])

# Remplacement des NaN par moyennes (features) ou suppression (cible)
for col in numeric_cols:
    if col != 'target_to':
        mean_val = features_df[col].mean()
        features_df[col] = features_df[col].fillna(mean_val)
```

## Algorithme XGBoost

### Configuration du mod√®le

```python
self.model = xgb.XGBRegressor(
    n_estimators=200,      # Nombre d'arbres dans la for√™t
    max_depth=6,           # Profondeur maximale des arbres
    learning_rate=0.1,     # Taux d'apprentissage
    subsample=0.8,         # Fraction des √©chantillons utilis√©s par arbre
    colsample_bytree=0.8,  # Fraction des features utilis√©s par arbre
    random_state=42,       # Reproductibilit√©
    n_jobs=-1             # Utilisation de tous les CPU disponibles
)
```

### Fonction de perte
**Objective :** `reg:squarederror` (r√©gression avec erreur quadratique)
**Raison :** Adapt√© pour pr√©dire des valeurs continues positives

### Optimisation
- **Boosting :** Gradient Boosting it√©ratif
- **R√©gularisation :** L1/L2 implicite via la structure des arbres
- **Early stopping :** Non utilis√© (ensemble fixe de 200 arbres)

## √âvaluation et m√©triques

### M√©triques principales

#### Mean Absolute Error (MAE)
```python
mae = mean_absolute_error(y_true, y_pred)
```
- **Interpr√©tation :** Erreur absolue moyenne en points de pourcentage
- **Exemple :** MAE = 0.05 signifie erreur moyenne de 5% sur le To

#### Root Mean Square Error (RMSE)
```python
rmse = np.sqrt(mean_squared_error(y_true, y_pred))
```
- **Interpr√©tation :** Racine de l'erreur quadratique moyenne
- **Sensibilit√© :** P√©nalise plus les grandes erreurs

#### Coefficient de d√©termination (R¬≤)
```python
r2 = r2_score(y_true, y_pred)
```
- **Interpr√©tation :** Pourcentage de variance expliqu√© par le mod√®le
- **Plage :** 0 √† 1 (1 = mod√®le parfait)

### Validation crois√©e

```python
cv_scores = cross_val_score(
    self.model, X_train_scaled, y_train,
    cv=5, scoring='neg_mean_absolute_error'
)
```
- **Strat√©gie :** 5-fold cross-validation
- **M√©trique :** Negative MAE (convention scikit-learn)
- **Interpr√©tation :** Robustesse du mod√®le sur diff√©rents sous-ensembles

## Normalisation des features

### StandardScaler
```python
self.scaler = StandardScaler()
X_scaled = self.scaler.fit_transform(X)
```

**Formule :** `X_scaled = (X - mean) / std`

- **Avantages :**
  - Features sur la m√™me √©chelle
  - Am√©liore la convergence de XGBoost
  - Pr√©serve les relations entre features

- **Features non normalis√©es :**
  - `cluster` : Variable cat√©gorielle (pas de normalisation)

## Gestion des donn√©es manquantes

### Strat√©gie principale
1. **Cible (target_to) :** Suppression des lignes avec NaN
2. **Features num√©riques :** Remplacement par la moyenne
3. **Features cat√©gorielles :** Remplacement par 0 ou mode

### Justification
- La cible ne peut pas √™tre estim√©e si elle est manquante
- Les features peuvent √™tre imput√©es sans biaiser excessivement le mod√®le
- Pr√©f√©rer la suppression √† l'imputation pour la cible (qualit√© > quantit√©)

## Sauvegarde et chargement du mod√®le

### Format de sauvegarde
```python
model_data = {
    'model': self.model,           # Mod√®le XGBoost entra√Æn√©
    'scaler': self.scaler,         # StandardScaler ajust√©
    'feature_columns': self.feature_columns  # Liste des features
}
joblib.dump(model_data, 'models/xgboost_to_predictor.pkl')
```

### Chargement
```python
model_data = joblib.load('models/xgboost_to_predictor.pkl')
predictor.model = model_data['model']
predictor.scaler = model_data['scaler']
predictor.feature_columns = model_data['feature_columns']
```

## Complexit√© algorithmique

### Entra√Ænement
- **Temps :** O(n_estimators √ó n_samples √ó max_depth √ó n_features)
- **M√©moire :** O(n_samples √ó n_features + n_estimators √ó n_nodes)

### Pr√©diction
- **Temps :** O(n_estimators √ó max_depth)
- **Tr√®s rapide** une fois le mod√®le entra√Æn√©

### Optimisations
- `n_jobs=-1` : Utilisation de tous les CPU disponibles
- `subsample=0.8` : R√©duction de la taille des √©chantillons par arbre
- `colsample_bytree=0.8` : R√©duction du nombre de features par arbre

## Limitations et am√©liorations possibles

### Limitations actuelles
1. **Donn√©es futures :** Approximation To = PM/RevPAR (pas de To r√©el)
2. **Horizon fixe :** Un mod√®le par horizon de pr√©diction
3. **Features √©conomiques :** Utilisation de valeurs uniques PM/RevPAR (pas de s√©ries temporelles)
4. **Features limit√©es :** Pas d'int√©gration de donn√©es externes (m√©t√©o, √©v√©nements)
5. **Cluster statique :** Utilise le cluster d√©termin√© historiquement

### Am√©liorations envisag√©es
1. **Multi-horizon :** Un seul mod√®le pour tous les horizons
2. **Features √©conomiques :** Ajout de s√©ries temporelles PM/RevPAR (au lieu de valeurs uniques)
3. **Features externes :** Int√©gration de donn√©es m√©t√©o, calendriers, √©v√©nements
4. **Cluster dynamique :** Classification automatique pour de nouveaux h√¥tels
5. **Probabiliste :** Pr√©diction d'intervalles de confiance
6. **S√©ries temporelles :** Utilisation d'algorithmes sp√©cialis√©s (LSTM, Prophet)

## D√©bogage et monitoring

### Logs d√©taill√©s
- Progression du chargement des donn√©es
- Nombre d'√©chantillons √† chaque √©tape
- M√©triques d'√©valuation compl√®tes
- Erreurs avec traceback complet

### Points de contr√¥le
- V√©rification de l'existence des fichiers
- Validation des types de donn√©es
- Contr√¥le de la qualit√© des features
- Tests de coh√©rence des pr√©dictions

## Utilisation en production

### Pr√©requis
- Python 3.7+
- pandas, numpy, scikit-learn, xgboost, matplotlib, seaborn, joblib
- Fichiers de donn√©es : `clustering_results.csv`, `Indicateurs.csv`

### D√©ploiement
```bash
# Entra√Ænement
python xgboost_to_prediction.py

# Utilisation du mod√®le entra√Æn√©
from xgboost_to_prediction import ToPredictor
predictor = ToPredictor.load_model('models/xgboost_to_predictor.pkl')
prediction = predictor.predict_future_to(hotel_code, current_date, curve_data, pm_current, revpar_current)
```

### Monitoring recommand√©
- Validation p√©riodique des performances
- R√©entra√Ænement avec nouvelles donn√©es
- Surveillance des distributions de pr√©dictions
- Alertes sur d√©rive de donn√©es (data drift)

## R√©f√©rences techniques

### XGBoost
- [Documentation officielle](https://xgboost.readthedocs.io/)
- [Guide des hyperparam√®tres](https://xgboost.readthedocs.io/en/latest/parameter.html)

### Scikit-learn
- [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)
- [Cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html)

### Pandas
- [Time series handling](https://pandas.pydata.org/docs/user_guide/timeseries.html)
- [DataFrame operations](https://pandas.pydata.org/docs/reference/frame.html)

---

*Document technique - Version 1.1 - R√©vis√© avec valeurs uniques PM/RevPAR - Date : D√©cembre 2024*
