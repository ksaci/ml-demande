# ğŸ¨ PrÃ©diction du Taux d'Occupation Futur avec XGBoost

Ce script utilise les rÃ©sultats du clustering pour entraÃ®ner un modÃ¨le XGBoost qui prÃ©dit le taux d'occupation (To) futur des hÃ´tels.

## ğŸ¯ Objectif

PrÃ©dire le taux d'occupation d'un hÃ´tel dans le futur (J+7, J+14, etc.) en utilisant :
- Les courbes de montÃ©e en charge clusterisÃ©es
- Les donnÃ©es PM (Prix Moyen) actuelles
- Les donnÃ©es RevPAR (Revenue Per Available Room) actuelles
- L'appartenance aux clusters identifiÃ©s

## ğŸ“Š DonnÃ©es d'entrÃ©e

### 1. RÃ©sultats du clustering (`results/clustering_results.csv`)
- **hotCode** : Code de l'hÃ´tel
- **stay_date** : Date de sÃ©jour
- **J-60 Ã  J-0** : Valeurs du taux d'occupation pour chaque jour avant la date de sÃ©jour
- **cluster** : NumÃ©ro du cluster assignÃ©

### 2. DonnÃ©es indicateurs (`data/*.csv`)
- **Pm** : Prix Moyen actuel
- **revpz** : RevPAR (Revenue Per Available Room) actuel

## ğŸš€ Utilisation

### EntraÃ®nement du modÃ¨le

```bash
cd c:\github\machineLearning\demande
python xgboost_to_prediction.py
```

### Configuration

Modifiez les paramÃ¨tres dans la fonction `main()` :

```python
# Fichier des rÃ©sultats de clustering
CLUSTERING_RESULTS = 'results/clustering_results.csv'

# Dossier contenant les fichiers indicateurs
INDICATEURS_DIR = 'data'

# Horizon de prÃ©diction (en jours)
PREDICTION_HORIZON = 7  # PrÃ©dire To Ã  J+7
```

## ğŸ—ï¸ Architecture du modÃ¨le

### Features utilisÃ©es
1. **Courbe de To rÃ©cente** : Valeurs J-7 Ã  J-37 (30 jours)
2. **Cluster** : Appartenance au cluster identifiÃ©
3. **PM actuel** : Prix Moyen du jour de sÃ©jour
4. **RevPAR actuel** : Revenue Per Available Room du jour

### Cible (Target)
- **To futur** : Taux d'occupation prÃ©dit Ã  J+`PREDICTION_HORIZON`

### Approches de prÃ©paration des donnÃ©es

#### 1. Approche principale
- Fusion directe des donnÃ©es clustering + PM/RevPAR
- Recherche des valeurs To futures dans les donnÃ©es historiques
- Calcul d'une approximation To = PM_futur / RevPAR_futur

#### 2. Approche alternative (si peu de donnÃ©es)
- Utilisation des patterns moyens par cluster
- PrÃ©diction basÃ©e sur l'Ã©volution typique du cluster

## ğŸ“ˆ Ã‰valuation du modÃ¨le

### MÃ©triques
- **MAE** (Mean Absolute Error) : Erreur absolue moyenne
- **RMSE** (Root Mean Square Error) : Racine de l'erreur quadratique moyenne
- **RÂ² Score** : Coefficient de dÃ©termination

### Validation croisÃ©e
- Validation 5-fold pour Ã©valuer la robustesse
- Comparaison train/test pour dÃ©tecter le surapprentissage

### Analyse des features importantes
- Graphique des 20 features les plus importantes
- SauvegardÃ© automatiquement dans `results/feature_importance.png`

## ğŸ’¾ Sauvegarde du modÃ¨le

Le modÃ¨le entraÃ®nÃ© est sauvegardÃ© dans :
```
models/xgboost_to_predictor.pkl
```

Contient :
- Le modÃ¨le XGBoost entraÃ®nÃ©
- Le scaler pour la normalisation
- La liste des features utilisÃ©es

## ğŸ”® Utilisation du modÃ¨le entraÃ®nÃ©

```python
from xgboost_to_prediction import ToPredictor

# Charger le modÃ¨le
predictor = ToPredictor.load_model('models/xgboost_to_predictor.pkl')

# PrÃ©parer les donnÃ©es de prÃ©diction
curve_data = {'J-7': 0.85, 'J-6': 0.87, 'J-5': 0.89, ...}  # 30 valeurs
pm_current = 120.5
revpar_current = 95.2

# PrÃ©dire
future_to = predictor.predict_future_to(
    hotel_code='ABC',
    current_date=pd.Timestamp('2024-01-15'),
    curve_data=curve_data,
    pm_current=pm_current,
    revpar_current=revpar_current,
    prediction_horizon=7
)

print(f"To prÃ©dit Ã  J+7 : {future_to:.3f}")
```

## âš™ï¸ Configuration XGBoost

```python
self.model = xgb.XGBRegressor(
    n_estimators=200,      # Nombre d'arbres
    max_depth=6,           # Profondeur maximale
    learning_rate=0.1,     # Taux d'apprentissage
    subsample=0.8,         # Fraction des Ã©chantillons
    colsample_bytree=0.8,  # Fraction des features
    random_state=42,
    n_jobs=-1             # Utilisation de tous les CPU
)
```

## ğŸ“ Structure des fichiers gÃ©nÃ©rÃ©s

```
demande/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ xgboost_to_predictor.pkl     # ModÃ¨le sauvegardÃ©
â”œâ”€â”€ results/
â”‚   â””â”€â”€ feature_importance.png       # Importance des features
â””â”€â”€ xgboost_to_prediction.py         # Script principal
```

## ğŸ”§ Personnalisation

### Changer l'horizon de prÃ©diction
```python
PREDICTION_HORIZON = 14  # PrÃ©dire Ã  J+14 au lieu de J+7
```

### Modifier les hyperparamÃ¨tres XGBoost
Ajustez les paramÃ¨tres dans la mÃ©thode `train_model()` pour optimiser les performances.

### Ajouter des features
Modifiez `prepare_features()` pour inclure d'autres variables prÃ©dictives (mÃ©tÃ©o, Ã©vÃ©nements, saisonnalitÃ©, etc.).

## ğŸ¯ Performance attendue

- **MAE typique** : 0.02 - 0.05 (2-5% d'erreur absolue)
- **RÂ² typique** : 0.75 - 0.90
- **Temps d'entraÃ®nement** : 2-5 minutes selon la taille des donnÃ©es

## ğŸš¨ DÃ©pannage

### Erreur "Aucune donnÃ©e cible trouvÃ©e"
- VÃ©rifiez que les dates dans `clustering_results.csv` correspondent aux dates dans `indicateurs.csv`
- Le script bascule automatiquement sur l'approche alternative

### Erreur "Peu d'Ã©chantillons"
- L'approche alternative est utilisÃ©e automatiquement
- ConsidÃ©rez rÃ©duire `PREDICTION_HORIZON` ou utiliser plus de donnÃ©es historiques

### ModÃ¨le qui overfit
- RÃ©duisez `max_depth` ou `n_estimators`
- Augmentez `subsample` et `colsample_bytree`
