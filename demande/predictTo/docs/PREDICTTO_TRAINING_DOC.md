# Documentation Technique - Entra√Ænement Mod√®le XGBoost

## üìã Vue d'ensemble

Ce document d√©crit le syst√®me d'entra√Ænement du mod√®le XGBoost pour la pr√©diction du taux d'occupation (TO) √† J+7.

**Version:** 1.0  
**Date:** D√©cembre 2024  
**Auteur:** √âquipe Data Science

---

## üéØ Objectif

Pr√©dire le taux d'occupation final (TO √† J+0) en utilisant :
- Les courbes de mont√©e en charge r√©centes (J-60 √† J-7)
- Le num√©ro de cluster assign√©
- Les features compress√©es des prix moyens (PM)
- Des features temporelles (mois, jour de la semaine)

---

## üèóÔ∏è Architecture du Code

### Structure des fichiers

```
demande/
‚îú‚îÄ‚îÄ predictTo/                       # Dossier du projet PredictTO
‚îÇ   ‚îú‚îÄ‚îÄ predictTo_train_model.py    # Script principal d'entra√Ænement
‚îÇ   ‚îú‚îÄ‚îÄ predictTo_predict_example.py # Exemple d'utilisation
‚îÇ   ‚îú‚îÄ‚îÄ test_predictTo_setup.py     # Validation environnement
‚îÇ   ‚îú‚îÄ‚îÄ load_predictTo_from_azure.py # Gestion Azure
‚îÇ   ‚îú‚îÄ‚îÄ config_predictTo.yaml       # Configuration
‚îÇ   ‚îú‚îÄ‚îÄ requirements_predictTo.txt  # D√©pendances
‚îÇ   ‚îú‚îÄ‚îÄ PREDICTTO_TRAINING_DOC.md   # Documentation technique
‚îÇ   ‚îú‚îÄ‚îÄ README.md                   # Guide principal
‚îÇ   ‚îî‚îÄ‚îÄ predictTo_training.log      # Logs d'ex√©cution
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ Indicateurs.csv             # Donn√©es PM/RevPAR
‚îî‚îÄ‚îÄ results/
    ‚îú‚îÄ‚îÄ clustering_results.csv      # R√©sultats du clustering
    ‚îú‚îÄ‚îÄ models/                     # Mod√®les sauvegard√©s
    ‚îú‚îÄ‚îÄ xgb_scatter_plot.png       # Visualisations
    ‚îî‚îÄ‚îÄ xgb_feature_importance.png
```

---

## üìä Pipeline de Donn√©es

### 1. Chargement des Donn√©es

**Sources:**
- `results/clustering_results.csv` : R√©sultats du clustering avec colonnes TO (J-60 √† J-0)
- `data/Indicateurs.csv` : Donn√©es des indicateurs PM et RevPAR

**Format attendu pour clustering_results.csv:**
```csv
hotCode;stay_date;nb_observations;J-60;J-59;...;J-0;cluster
D09;2022-01-01;61;0.213077;0.218891;...;0.665601;4
```

**Format attendu pour Indicateurs.csv:**
```csv
hotCode;Date;ObsDate;Pm;RevPAR
D09;2022-01-01;2021-11-02;148.48;95.2
```

### 2. Pr√©paration des Donn√©es

#### √âtape 2.1 : Calcul de la distance temporelle
```python
indicateurs["days_before"] = (indicateurs["Date"] - indicateurs["ObsDate"]).dt.days
```

#### √âtape 2.2 : Pivot des PM
Transformation des observations PM en colonnes `pm_J-0`, `pm_J-1`, ..., `pm_J-60`

#### √âtape 2.3 : Calcul des features PM compress√©es
√Ä partir de la s√©rie temporelle PM (J-60 ‚Üí J-8), on calcule :

| Feature | Description | Formule |
|---------|-------------|---------|
| `pm_mean` | Prix moyen | `mean(PM_series)` |
| `pm_slope` | Pente de la tendance | R√©gression lin√©aire |
| `pm_volatility` | Volatilit√© | `std(PM_series)` |
| `pm_diff_sum` | Somme des variations | `sum(abs(diff(PM_series)))` |
| `pm_change_ratio` | Ratio de changement | `(PM_last - PM_first) / PM_first` |
| `pm_last_jump` | Variation r√©cente | `PM_last - PM[-6]` |
| `pm_trend_changes` | Nb changements de direction | Comptage des inversions de tendance |

#### √âtape 2.4 : Features temporelles
```python
df["month"] = df["stay_date"].dt.month          # Mois (1-12)
df["dayofweek"] = df["stay_date"].dt.dayofweek  # Jour semaine (0-6)
```

### 3. Construction des Features

**Liste compl√®te des features:**

1. **TO historiques (53 features):** `J-60`, `J-59`, ..., `J-8`
2. **PM compress√©es (7 features):** 
   - `pm_mean`, `pm_slope`, `pm_volatility`, `pm_diff_sum`
   - `pm_change_ratio`, `pm_last_jump`, `pm_trend_changes`
3. **Features additionnelles (4 features):**
   - `nb_observations` : Nombre d'observations
   - `cluster` : Num√©ro de cluster (0-N)
   - `month` : Mois du s√©jour
   - `dayofweek` : Jour de la semaine

**Total : 64 features**

**Variable cible:** `J-0` (Taux d'occupation final)

---

## ü§ñ Mod√®le XGBoost

### Configuration par d√©faut

```python
{
    'n_estimators': 600,        # Nombre d'arbres
    'learning_rate': 0.05,      # Taux d'apprentissage
    'max_depth': 7,             # Profondeur max des arbres
    'subsample': 0.9,           # √âchantillonnage des lignes
    'colsample_bytree': 0.9,    # √âchantillonnage des colonnes
    'min_child_weight': 1,      # Poids minimum des feuilles
    'reg_lambda': 1.0,          # R√©gularisation L2
    'n_jobs': -1,               # Utiliser tous les CPU
    'random_state': 42          # Reproductibilit√©
}
```

### Pr√©traitement

**Normalisation StandardScaler:**
```python
X_scaled = StandardScaler().fit_transform(X)
```
- Moyenne = 0
- √âcart-type = 1
- Appliqu√© sur toutes les features

### Split Train/Test

- **Train:** 80% des donn√©es
- **Test:** 20% des donn√©es
- **M√©thode:** Split al√©atoire stratifi√© (`random_state=42`)

---

## üìà M√©triques d'√âvaluation

### M√©triques calcul√©es

1. **MAE (Mean Absolute Error)**
   ```
   MAE = mean(|y_true - y_pred|)
   ```
   - Erreur moyenne en points de TO
   - Exemple : MAE = 0.056 ‚Üí erreur moyenne de 5.6%

2. **RMSE (Root Mean Squared Error)**
   ```
   RMSE = sqrt(mean((y_true - y_pred)¬≤))
   ```
   - P√©nalise plus fortement les grandes erreurs

3. **R¬≤ (Coefficient de d√©termination)**
   ```
   R¬≤ = 1 - (SS_res / SS_tot)
   ```
   - Proportion de variance expliqu√©e
   - R¬≤ = 0.83 ‚Üí 83% de la variance expliqu√©e

### R√©sultats attendus

| M√©trique | Train | Test |
|----------|-------|------|
| MAE | ~0.045 | ~0.056 |
| RMSE | ~0.062 | ~0.075 |
| R¬≤ | ~0.89 | ~0.83 |

### Feature Importance

Les features les plus importantes sont g√©n√©ralement :
1. `J-8` (TO √† J-8) : ~46%
2. `J-9` (TO √† J-9) : ~13%
3. `cluster` : ~8%
4. `J-23` : ~3%
5. `pm_change_ratio` : ~1%

---

## ‚òÅÔ∏è Sauvegarde Azure Blob Storage

### Configuration requise

**Variable d'environnement:**
```bash
export AZURE_STORAGE_CONNECTION_STRING="DefaultEndpointsProtocol=https;AccountName=...;AccountKey=...;EndpointSuffix=core.windows.net"
```

### Structure de sauvegarde

```
Container: prediction-demande
‚îî‚îÄ‚îÄ models/
    ‚îî‚îÄ‚îÄ 20241216_143025/          # Timestamp de l'entra√Ænement
        ‚îú‚îÄ‚îÄ xgb_to_predictor.joblib
        ‚îú‚îÄ‚îÄ xgb_scaler.joblib
        ‚îî‚îÄ‚îÄ feature_columns.txt
```

### Fichiers sauvegard√©s

1. **xgb_to_predictor.joblib**
   - Mod√®le XGBoost entra√Æn√©
   - Format : joblib (pickle optimis√©)
   - Taille : ~2-5 MB

2. **xgb_scaler.joblib**
   - StandardScaler ajust√©
   - Contient les param√®tres de normalisation

3. **feature_columns.txt**
   - Liste ordonn√©e des features
   - Crucial pour la pr√©diction

---

## üöÄ Utilisation

### Installation des d√©pendances

```bash
pip install pandas numpy scikit-learn xgboost joblib matplotlib seaborn azure-storage-blob
```

### Ex√©cution du script

```bash
# M√©thode 1 : Ex√©cution directe
python demande/xgboost_train_model.py

# M√©thode 2 : Avec variables d'environnement
export AZURE_STORAGE_CONNECTION_STRING="..."
python demande/xgboost_train_model.py
```

### Utilisation programmatique

```python
from xgboost_train_model import XGBoostOccupancyPredictor

# Configuration
config = {
    'clustering_results_path': 'results/clustering_results.csv',
    'indicateurs_path': 'data/Indicateurs.csv',
    'prediction_horizon': 7,
    'test_size': 0.2,
    'random_state': 42
}

# Instanciation
predictor = XGBoostOccupancyPredictor(config)

# Pipeline complet
clusters, indicateurs = predictor.load_data()
df = predictor.prepare_data(clusters, indicateurs)
X, y = predictor.create_features_target(df)
results = predictor.train_model(X, y)
predictor.evaluate_model(save_plots=True)
predictor.save_model_locally()
```

---

## üîç Monitoring et Logs

### Fichier de logs

**Emplacement:** `xgboost_training.log`

**Format:**
```
2024-12-16 14:30:25 - __main__ - INFO - Initialisation du XGBoostOccupancyPredictor
2024-12-16 14:30:26 - __main__ - INFO - Chargement des donn√©es...
2024-12-16 14:30:28 - __main__ - INFO - Clusters charg√©s: (1415, 65)
```

### Informations logg√©es

- ‚úÖ √âtapes du pipeline
- üìä M√©triques d'entra√Ænement
- ‚ö†Ô∏è Warnings (donn√©es manquantes, etc.)
- ‚ùå Erreurs avec stack trace complet

---

## üõ†Ô∏è Maintenance et √âvolution

### R√©entra√Ænement du mod√®le

**Fr√©quence recommand√©e:** Mensuelle ou lorsque :
- Nouvelles donn√©es disponibles (> 10% volume)
- Performance d√©grad√©e (MAE > seuil)
- Changement de saisonnalit√©

### Ajout de nouvelles features

1. Modifier `create_features_target()` pour ajouter les features
2. Mettre √† jour `feature_cols`
3. R√©-entra√Æner le mod√®le
4. Comparer les performances

### Optimisation des hyperparam√®tres

Utiliser `GridSearchCV` ou `RandomizedSearchCV` :

```python
from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [400, 600, 800],
    'learning_rate': [0.01, 0.05, 0.1],
    'max_depth': [5, 7, 9]
}

grid_search = GridSearchCV(
    xgb.XGBRegressor(),
    param_grid,
    cv=5,
    scoring='neg_mean_absolute_error',
    n_jobs=-1
)
```

---

## üìö R√©f√©rences

- [XGBoost Documentation](https://xgboost.readthedocs.io/)
- [Scikit-learn Preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html)
- [Azure Blob Storage Python SDK](https://docs.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blobs-python)

---

## ‚ö†Ô∏è Points d'attention

### Gestion des valeurs manquantes

- Les NaN dans les s√©ries PM sont g√©r√©s par `compute_pm_features()`
- Les lignes avec NaN dans les features sont supprim√©es avant l'entra√Ænement

### Reproductibilit√©

- Utiliser toujours le m√™me `random_state`
- V√©rifier que les donn√©es sources sont identiques
- Conserver les versions des biblioth√®ques

### Performance

- Temps d'entra√Ænement : ~30-60 secondes (selon CPU)
- Taille du mod√®le : ~2-5 MB
- Temps de pr√©diction : <1ms par observation

---

## üêõ R√©solution de probl√®mes

### Erreur : "AZURE_STORAGE_CONNECTION_STRING non d√©finie"

**Solution:** D√©finir la variable d'environnement ou ignorer la sauvegarde Azure

```bash
export AZURE_STORAGE_CONNECTION_STRING="votre_connection_string"
```

### Erreur : "La colonne 'J-0' est absente"

**Cause:** Fichier `clustering_results.csv` mal format√©  
**Solution:** V√©rifier le format du CSV (s√©parateur `;`, colonnes TO pr√©sentes)

### Performance d√©grad√©e (R¬≤ < 0.7)

**Causes possibles:**
- Donn√©es insuffisantes
- Distribution diff√©rente (concept drift)
- Features manquantes

**Actions:**
- Augmenter le nombre de donn√©es
- V√©rifier la qualit√© des clusters
- Ajouter des features pertinentes

---

## üìû Contact

Pour toute question technique, contacter l'√©quipe Data Science.

---

**Derni√®re mise √† jour:** 16 D√©cembre 2024

